{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys, gc, time, warnings, pickle, psutil, random\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# :seed to make all processes deterministic     # type: int\n",
    "def seed_everything(seed=0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "def get_data_by_dept(dept_id):\n",
    "    \n",
    "    # Read and contact basic feature\n",
    "    df = pd.concat([pd.read_pickle(BASE),\n",
    "                    pd.read_pickle(PRICE).iloc[:,2:],\n",
    "                    pd.read_pickle(CALENDAR).iloc[:,2:]],\n",
    "                    axis=1)\n",
    "    \n",
    "    # Leave only relevant store\n",
    "    df = df[df['dept_id']==dept_id]\n",
    "\n",
    "    # With memory limits we have to read \n",
    "    # lags and mean encoding features\n",
    "    # separately and drop items that we don't need.\n",
    "    # As our Features Grids are aligned \n",
    "    # we can use index to keep only necessary rows\n",
    "    # Alignment is good for us as concat uses less memory than merge.\n",
    "    df2 = pd.read_pickle(MEAN_ENC)[mean_features]\n",
    "    df2 = df2[df2.index.isin(df.index)]\n",
    "    \n",
    "    df3 = pd.read_pickle(LAGS).iloc[:,3:]\n",
    "    df3 = df3[df3.index.isin(df.index)]\n",
    "    \n",
    "    df = pd.concat([df, df2], axis=1)\n",
    "    del df2 # to not reach memory limit \n",
    "    \n",
    "    df = pd.concat([df, df3], axis=1)\n",
    "    del df3 # to not reach memory limit \n",
    "    \n",
    "    # Create features list\n",
    "    features = [col for col in list(df) if col not in remove_features]\n",
    "    df = df[['id','d',TARGET]+features]\n",
    "    \n",
    "    # Skipping first n rows\n",
    "    df = df[df['d']>=START_TRAIN].reset_index(drop=True)\n",
    "    \n",
    "    return df, features\n",
    "\n",
    "# Recombine Test set after training\n",
    "def get_base_test():\n",
    "    base_test = pd.DataFrame()\n",
    "\n",
    "    for dept_id in DEPT_IDS:\n",
    "        temp_df = pd.read_pickle('test_'+dept_id+'.pkl')\n",
    "        temp_df['dept_id'] = dept_id\n",
    "        base_test = pd.concat([base_test, temp_df]).reset_index(drop=True)\n",
    "    \n",
    "    return base_test\n",
    "\n",
    "\n",
    "########################### Helper to make dynamic rolling lags\n",
    "#################################################################################\n",
    "def make_lag(LAG_DAY):\n",
    "    lag_df = base_test[['id','d',TARGET]]\n",
    "    col_name = 'sales_lag_'+str(LAG_DAY)\n",
    "    lag_df[col_name] = lag_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(LAG_DAY)).astype(np.float16)\n",
    "    return lag_df[[col_name]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lag_roll(LAG_DAY,lag_df_new):\n",
    "   \n",
    "    lag_df = base_test[['id','d',TARGET]]\n",
    "    \n",
    "    lag_df=lag_df.sort_values(by=[\"d\"])\n",
    "  \n",
    "    for i in range(0,len(LAG_DAY)):\n",
    "\n",
    "        shift_day = LAG_DAY[i][0]\n",
    "        roll_wind = LAG_DAY[i][1]\n",
    "        col_name = 'rolling_mean_tmp_'+str(shift_day)+'_'+str(roll_wind)\n",
    "        lag_df[col_name] = (lag_df.groupby(['id'])[TARGET]).transform(lambda x: x.shift(shift_day).rolling(roll_wind).mean())\n",
    "    lag_df_new=lag_df.drop(columns=[\"sales\"])\n",
    "    return lag_df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "lgb_params = {\n",
    "                    'boosting_type': 'gbdt',\n",
    "                    'objective': 'tweedie',\n",
    "                    'tweedie_variance_power': 1.1,\n",
    "                    'metric': 'rmse',\n",
    "                    'subsample': 0.5,\n",
    "                    'subsample_freq': 1,\n",
    "                    'learning_rate': 0.03,\n",
    "                    \"lambda\":0.1,\n",
    "                    'num_leaves': 2**11-1,\n",
    "                    'min_data_in_leaf': 2**12-1,\n",
    "                    'feature_fraction': 0.5,\n",
    "                    'max_bin': 100,\n",
    "                    'n_estimators': 1400,\n",
    "                    'boost_from_average': False,\n",
    "                    'verbose': -1,\n",
    "                } \n",
    "\n",
    "\n",
    "\n",
    "# lgb_params ={\n",
    "#         \"objective\" : \"tweedie\",\n",
    "#         \"metric\" :\"rmse\",\n",
    "#         \"force_row_wise\" : True,\n",
    "#         \"learning_rate\" : 0.075,\n",
    "#         \"sub_feature\" : 0.8,\n",
    "#         \"sub_row\" : 0.75,\n",
    "#         \"bagging_freq\" : 1,\n",
    "#         \"lambda_l2\" : 0.1,\n",
    "#         \"metric\": [\"rmse\"],\n",
    "#         \"nthread\": -1,\n",
    "#         \"tweedie_variance_power\":1.1,\n",
    "#     'verbosity': 1,\n",
    "# #     'num_iterations' : 1500,\n",
    "#     'num_leaves': 128,\n",
    "#     \"min_data_in_leaf\": 104,\n",
    "#     }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Let's look closer on params\n",
    "\n",
    "## 'boosting_type': 'gbdt'\n",
    "# we have 'goss' option for faster training\n",
    "# but it normally leads to underfit.\n",
    "# Also there is good 'dart' mode\n",
    "# but it takes forever to train\n",
    "# and model performance depends \n",
    "# a lot on random factor \n",
    "# https://www.kaggle.com/c/home-credit-default-risk/discussion/60921\n",
    "\n",
    "## 'objective': 'tweedie'\n",
    "# Tweedie Gradient Boosting for Extremely\n",
    "# Unbalanced Zero-inflated Data\n",
    "# https://arxiv.org/pdf/1811.10192.pdf\n",
    "# and many more articles about tweediie\n",
    "#\n",
    "# Strange (for me) but Tweedie is close in results\n",
    "# to my own ugly loss.\n",
    "# My advice here - make OWN LOSS function\n",
    "# https://www.kaggle.com/c/m5-forecasting-accuracy/discussion/140564\n",
    "# https://www.kaggle.com/c/m5-forecasting-accuracy/discussion/143070\n",
    "# I think many of you already using it (after poisson kernel appeared) \n",
    "# (kagglers are very good with \"params\" testing and tuning).\n",
    "# Try to figure out why Tweedie works.\n",
    "# probably it will show you new features options\n",
    "# or data transformation (Target transformation?).\n",
    "\n",
    "## 'tweedie_variance_power': 1.1\n",
    "# default = 1.5\n",
    "# set this closer to 2 to shift towards a Gamma distribution\n",
    "# set this closer to 1 to shift towards a Poisson distribution\n",
    "# my CV shows 1.1 is optimal \n",
    "# but you can make your own choice\n",
    "\n",
    "## 'metric': 'rmse'\n",
    "# Doesn't mean anything to us\n",
    "# as competition metric is different\n",
    "# and we don't use early stoppings here.\n",
    "# So rmse serves just for general \n",
    "# model performance overview.\n",
    "# Also we use \"fake\" validation set\n",
    "# (as it makes part of the training set)\n",
    "# so even general rmse score doesn't mean anything))\n",
    "# https://www.kaggle.com/c/m5-forecasting-accuracy/discussion/133834\n",
    "\n",
    "## 'subsample': 0.5\n",
    "# Serves to fight with overfit\n",
    "# this will randomly select part of data without resampling\n",
    "# Chosen by CV (my CV can be wrong!)\n",
    "# Next kernel will be about CV\n",
    "\n",
    "##'subsample_freq': 1\n",
    "# frequency for bagging\n",
    "# default value - seems ok\n",
    "\n",
    "## 'learning_rate': 0.03\n",
    "# Chosen by CV\n",
    "# Smaller - longer training\n",
    "# but there is an option to stop \n",
    "# in \"local minimum\"\n",
    "# Bigger - faster training\n",
    "# but there is a chance to\n",
    "# not find \"global minimum\" minimum\n",
    "\n",
    "## 'num_leaves': 2**11-1\n",
    "## 'min_data_in_leaf': 2**12-1\n",
    "# Force model to use more features\n",
    "# We need it to reduce \"recursive\"\n",
    "# error impact.\n",
    "# Also it leads to overfit\n",
    "# that's why we use small \n",
    "\n",
    "# 'max_bin': 100\n",
    "## l1, l2 regularizations\n",
    "# https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c\n",
    "# Good tiny explanation\n",
    "# l2 can work with bigger num_leaves\n",
    "# but my CV doesn't show boost\n",
    "                    \n",
    "## 'n_estimators': 1400\n",
    "# CV shows that there should be\n",
    "# different values for each state/store.\n",
    "# Current value was chosen \n",
    "# for general purpose.\n",
    "# As we don't use any early stopings\n",
    "# careful to not overfit Public LB.\n",
    "\n",
    "##'feature_fraction': 0.5\n",
    "# LightGBM will randomly select \n",
    "# part of features on each iteration (tree).\n",
    "# We have maaaany features\n",
    "# and many of them are \"duplicates\"\n",
    "# and many just \"noise\"\n",
    "# good values here - 0.5-0.7 (by CV)\n",
    "\n",
    "## 'boost_from_average': False\n",
    "# There is some \"problem\"\n",
    "# to code boost_from_average for \n",
    "# custom loss\n",
    "# 'True' makes training faster\n",
    "# BUT carefull use it\n",
    "# https://github.com/microsoft/LightGBM/issues/1514"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "VER = 1                          # Our model version\n",
    "SEED = 666                      # We want all things\n",
    "seed_everything(SEED)            # to be as deterministic \n",
    "lgb_params['seed'] = SEED        # as possible\n",
    "N_CORES = psutil.cpu_count()     # Available CPU cores\n",
    "\n",
    "\n",
    "#LIMITS and const\n",
    "TARGET      = 'sales'            # Our target\n",
    "START_TRAIN = 0                  # We can skip some rows (Nans/faster training)\n",
    "END_TRAIN   = 1913               # End day of our train set\n",
    "P_HORIZON   = 28                 # Prediction horizon\n",
    "\n",
    "#FEATURES to remove\n",
    "## These features lead to overfit\n",
    "## or values not present in test set\n",
    "remove_features = ['id','dept_id','cat_id',\n",
    "                   'date','wm_yr_wk','d',TARGET]\n",
    "mean_features   = ['enc_state_id_mean','enc_state_id_std',\n",
    "                   'enc_store_id_mean','enc_store_id_std',\n",
    "                   'enc_item_id_mean','enc_item_id_std'] \n",
    "\n",
    "#PATHS for Features\n",
    "BASE     = 'grid_part_1.pkl'\n",
    "PRICE    = 'grid_part_2.pkl'\n",
    "CALENDAR = 'grid_part_3.pkl'\n",
    "LAGS     = 'lags_df_28.pkl'\n",
    "MEAN_ENC = 'mean_encoding_df.pkl'\n",
    "\n",
    "\n",
    "# AUX(pretrained) Models paths\n",
    "\n",
    "#STORES ids\n",
    "DEPT_IDS = pd.read_csv('sales_train_validation.csv')['dept_id']\n",
    "DEPT_IDS  = list(DEPT_IDS.unique())\n",
    "\n",
    "#SPLITS for lags creation\n",
    "SHIFT_DAY  = 28\n",
    "N_LAGS     = 15\n",
    "LAGS_SPLIT = [col for col in range(SHIFT_DAY,SHIFT_DAY+N_LAGS)]\n",
    "ROLS_SPLIT = []\n",
    "for i in [1,7,14]:\n",
    "    for j in [7,14,28,56]:\n",
    "        ROLS_SPLIT.append([i,j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train HOBBIES_1\n",
      "['item_id', 'store_id', 'state_id', 'release', 'sell_price', 'price_max', 'price_min', 'price_std', 'price_mean', 'price_norm', 'price_rank_dept', 'price_nunique', 'item_nunique', 'price_momentum', 'price_momentum_m', 'price_momentum_y', 'temperature_high', 'temperature_con', 'rainfall_m', 'snow_m', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', 'snap_CA', 'snap_TX', 'snap_WI', 'is_first_half_month', 'event_bef_weekend', 'event_after_weekend', 'NBA', 'event_attention_after', 'event_attention_bef', 'event_attention_sum', 'tm_d', 'tm_w', 'tm_m', 'tm_q', 'tm_y', 'tm_wm', 'tm_dw', 'tm_w_end', 'enc_state_id_mean', 'enc_state_id_std', 'enc_store_id_mean', 'enc_store_id_std', 'enc_item_id_mean', 'enc_item_id_std', 'sales_lag_28', 'sales_lag_29', 'sales_lag_30', 'sales_lag_31', 'sales_lag_32', 'sales_lag_33', 'sales_lag_34', 'sales_lag_35', 'sales_lag_36', 'sales_lag_37', 'sales_lag_38', 'sales_lag_39', 'sales_lag_40', 'sales_lag_41', 'sales_lag_42', 'rolling_mean_7', 'rolling_std_7', 'rolling_mean_14', 'rolling_std_14', 'rolling_mean_28', 'rolling_std_28', 'rolling_mean_56', 'rolling_std_56', 'rolling_mean_168', 'rolling_std_168', 'rolling_mean_tmp_1_7', 'rolling_mean_tmp_1_14', 'rolling_mean_tmp_1_28', 'rolling_mean_tmp_1_56', 'rolling_mean_tmp_7_7', 'rolling_mean_tmp_7_14', 'rolling_mean_tmp_7_28', 'rolling_mean_tmp_7_56', 'rolling_mean_tmp_14_7', 'rolling_mean_tmp_14_14', 'rolling_mean_tmp_14_28', 'rolling_mean_tmp_14_56']\n",
      "[100]\tvalid_0's rmse: 1.85108\n",
      "[200]\tvalid_0's rmse: 1.84083\n",
      "[300]\tvalid_0's rmse: 1.83565\n",
      "[400]\tvalid_0's rmse: 1.83094\n",
      "[500]\tvalid_0's rmse: 1.82591\n",
      "[600]\tvalid_0's rmse: 1.82233\n",
      "[700]\tvalid_0's rmse: 1.81828\n",
      "[800]\tvalid_0's rmse: 1.81413\n",
      "[900]\tvalid_0's rmse: 1.81026\n",
      "[1000]\tvalid_0's rmse: 1.80675\n",
      "[1100]\tvalid_0's rmse: 1.8035\n",
      "[1200]\tvalid_0's rmse: 1.80029\n",
      "[1300]\tvalid_0's rmse: 1.7975\n",
      "[1400]\tvalid_0's rmse: 1.79432\n",
      "Train HOBBIES_2\n",
      "['item_id', 'store_id', 'state_id', 'release', 'sell_price', 'price_max', 'price_min', 'price_std', 'price_mean', 'price_norm', 'price_rank_dept', 'price_nunique', 'item_nunique', 'price_momentum', 'price_momentum_m', 'price_momentum_y', 'temperature_high', 'temperature_con', 'rainfall_m', 'snow_m', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', 'snap_CA', 'snap_TX', 'snap_WI', 'is_first_half_month', 'event_bef_weekend', 'event_after_weekend', 'NBA', 'event_attention_after', 'event_attention_bef', 'event_attention_sum', 'tm_d', 'tm_w', 'tm_m', 'tm_q', 'tm_y', 'tm_wm', 'tm_dw', 'tm_w_end', 'enc_state_id_mean', 'enc_state_id_std', 'enc_store_id_mean', 'enc_store_id_std', 'enc_item_id_mean', 'enc_item_id_std', 'sales_lag_28', 'sales_lag_29', 'sales_lag_30', 'sales_lag_31', 'sales_lag_32', 'sales_lag_33', 'sales_lag_34', 'sales_lag_35', 'sales_lag_36', 'sales_lag_37', 'sales_lag_38', 'sales_lag_39', 'sales_lag_40', 'sales_lag_41', 'sales_lag_42', 'rolling_mean_7', 'rolling_std_7', 'rolling_mean_14', 'rolling_std_14', 'rolling_mean_28', 'rolling_std_28', 'rolling_mean_56', 'rolling_std_56', 'rolling_mean_168', 'rolling_std_168', 'rolling_mean_tmp_1_7', 'rolling_mean_tmp_1_14', 'rolling_mean_tmp_1_28', 'rolling_mean_tmp_1_56', 'rolling_mean_tmp_7_7', 'rolling_mean_tmp_7_14', 'rolling_mean_tmp_7_28', 'rolling_mean_tmp_7_56', 'rolling_mean_tmp_14_7', 'rolling_mean_tmp_14_14', 'rolling_mean_tmp_14_28', 'rolling_mean_tmp_14_56']\n",
      "[100]\tvalid_0's rmse: 0.748951\n",
      "[200]\tvalid_0's rmse: 0.745814\n",
      "[300]\tvalid_0's rmse: 0.743355\n",
      "[400]\tvalid_0's rmse: 0.741432\n",
      "[500]\tvalid_0's rmse: 0.739617\n",
      "[600]\tvalid_0's rmse: 0.73813\n",
      "[700]\tvalid_0's rmse: 0.73648\n",
      "[800]\tvalid_0's rmse: 0.734572\n",
      "[900]\tvalid_0's rmse: 0.732853\n",
      "[1000]\tvalid_0's rmse: 0.731511\n",
      "[1100]\tvalid_0's rmse: 0.729867\n",
      "[1200]\tvalid_0's rmse: 0.728492\n",
      "[1300]\tvalid_0's rmse: 0.726998\n",
      "[1400]\tvalid_0's rmse: 0.72561\n",
      "Train HOUSEHOLD_1\n",
      "['item_id', 'store_id', 'state_id', 'release', 'sell_price', 'price_max', 'price_min', 'price_std', 'price_mean', 'price_norm', 'price_rank_dept', 'price_nunique', 'item_nunique', 'price_momentum', 'price_momentum_m', 'price_momentum_y', 'temperature_high', 'temperature_con', 'rainfall_m', 'snow_m', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', 'snap_CA', 'snap_TX', 'snap_WI', 'is_first_half_month', 'event_bef_weekend', 'event_after_weekend', 'NBA', 'event_attention_after', 'event_attention_bef', 'event_attention_sum', 'tm_d', 'tm_w', 'tm_m', 'tm_q', 'tm_y', 'tm_wm', 'tm_dw', 'tm_w_end', 'enc_state_id_mean', 'enc_state_id_std', 'enc_store_id_mean', 'enc_store_id_std', 'enc_item_id_mean', 'enc_item_id_std', 'sales_lag_28', 'sales_lag_29', 'sales_lag_30', 'sales_lag_31', 'sales_lag_32', 'sales_lag_33', 'sales_lag_34', 'sales_lag_35', 'sales_lag_36', 'sales_lag_37', 'sales_lag_38', 'sales_lag_39', 'sales_lag_40', 'sales_lag_41', 'sales_lag_42', 'rolling_mean_7', 'rolling_std_7', 'rolling_mean_14', 'rolling_std_14', 'rolling_mean_28', 'rolling_std_28', 'rolling_mean_56', 'rolling_std_56', 'rolling_mean_168', 'rolling_std_168', 'rolling_mean_tmp_1_7', 'rolling_mean_tmp_1_14', 'rolling_mean_tmp_1_28', 'rolling_mean_tmp_1_56', 'rolling_mean_tmp_7_7', 'rolling_mean_tmp_7_14', 'rolling_mean_tmp_7_28', 'rolling_mean_tmp_7_56', 'rolling_mean_tmp_14_7', 'rolling_mean_tmp_14_14', 'rolling_mean_tmp_14_28', 'rolling_mean_tmp_14_56']\n",
      "[100]\tvalid_0's rmse: 1.75689\n",
      "[200]\tvalid_0's rmse: 1.7318\n",
      "[300]\tvalid_0's rmse: 1.7228\n",
      "[400]\tvalid_0's rmse: 1.7166\n",
      "[500]\tvalid_0's rmse: 1.71125\n",
      "[600]\tvalid_0's rmse: 1.70627\n",
      "[700]\tvalid_0's rmse: 1.7017\n",
      "[800]\tvalid_0's rmse: 1.6974\n",
      "[900]\tvalid_0's rmse: 1.69346\n",
      "[1000]\tvalid_0's rmse: 1.68956\n",
      "[1100]\tvalid_0's rmse: 1.6856\n",
      "[1200]\tvalid_0's rmse: 1.682\n",
      "[1300]\tvalid_0's rmse: 1.67834\n",
      "[1400]\tvalid_0's rmse: 1.67505\n",
      "Train HOUSEHOLD_2\n",
      "['item_id', 'store_id', 'state_id', 'release', 'sell_price', 'price_max', 'price_min', 'price_std', 'price_mean', 'price_norm', 'price_rank_dept', 'price_nunique', 'item_nunique', 'price_momentum', 'price_momentum_m', 'price_momentum_y', 'temperature_high', 'temperature_con', 'rainfall_m', 'snow_m', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', 'snap_CA', 'snap_TX', 'snap_WI', 'is_first_half_month', 'event_bef_weekend', 'event_after_weekend', 'NBA', 'event_attention_after', 'event_attention_bef', 'event_attention_sum', 'tm_d', 'tm_w', 'tm_m', 'tm_q', 'tm_y', 'tm_wm', 'tm_dw', 'tm_w_end', 'enc_state_id_mean', 'enc_state_id_std', 'enc_store_id_mean', 'enc_store_id_std', 'enc_item_id_mean', 'enc_item_id_std', 'sales_lag_28', 'sales_lag_29', 'sales_lag_30', 'sales_lag_31', 'sales_lag_32', 'sales_lag_33', 'sales_lag_34', 'sales_lag_35', 'sales_lag_36', 'sales_lag_37', 'sales_lag_38', 'sales_lag_39', 'sales_lag_40', 'sales_lag_41', 'sales_lag_42', 'rolling_mean_7', 'rolling_std_7', 'rolling_mean_14', 'rolling_std_14', 'rolling_mean_28', 'rolling_std_28', 'rolling_mean_56', 'rolling_std_56', 'rolling_mean_168', 'rolling_std_168', 'rolling_mean_tmp_1_7', 'rolling_mean_tmp_1_14', 'rolling_mean_tmp_1_28', 'rolling_mean_tmp_1_56', 'rolling_mean_tmp_7_7', 'rolling_mean_tmp_7_14', 'rolling_mean_tmp_7_28', 'rolling_mean_tmp_7_56', 'rolling_mean_tmp_14_7', 'rolling_mean_tmp_14_14', 'rolling_mean_tmp_14_28', 'rolling_mean_tmp_14_56']\n",
      "[100]\tvalid_0's rmse: 0.794144\n",
      "[200]\tvalid_0's rmse: 0.770706\n",
      "[300]\tvalid_0's rmse: 0.760232\n",
      "[400]\tvalid_0's rmse: 0.754316\n",
      "[500]\tvalid_0's rmse: 0.751091\n",
      "[600]\tvalid_0's rmse: 0.748535\n",
      "[700]\tvalid_0's rmse: 0.746153\n",
      "[800]\tvalid_0's rmse: 0.744077\n",
      "[900]\tvalid_0's rmse: 0.742169\n",
      "[1000]\tvalid_0's rmse: 0.740481\n",
      "[1100]\tvalid_0's rmse: 0.738829\n",
      "[1200]\tvalid_0's rmse: 0.737414\n",
      "[1300]\tvalid_0's rmse: 0.735738\n",
      "[1400]\tvalid_0's rmse: 0.734172\n",
      "Train FOODS_1\n",
      "['item_id', 'store_id', 'state_id', 'release', 'sell_price', 'price_max', 'price_min', 'price_std', 'price_mean', 'price_norm', 'price_rank_dept', 'price_nunique', 'item_nunique', 'price_momentum', 'price_momentum_m', 'price_momentum_y', 'temperature_high', 'temperature_con', 'rainfall_m', 'snow_m', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', 'snap_CA', 'snap_TX', 'snap_WI', 'is_first_half_month', 'event_bef_weekend', 'event_after_weekend', 'NBA', 'event_attention_after', 'event_attention_bef', 'event_attention_sum', 'tm_d', 'tm_w', 'tm_m', 'tm_q', 'tm_y', 'tm_wm', 'tm_dw', 'tm_w_end', 'enc_state_id_mean', 'enc_state_id_std', 'enc_store_id_mean', 'enc_store_id_std', 'enc_item_id_mean', 'enc_item_id_std', 'sales_lag_28', 'sales_lag_29', 'sales_lag_30', 'sales_lag_31', 'sales_lag_32', 'sales_lag_33', 'sales_lag_34', 'sales_lag_35', 'sales_lag_36', 'sales_lag_37', 'sales_lag_38', 'sales_lag_39', 'sales_lag_40', 'sales_lag_41', 'sales_lag_42', 'rolling_mean_7', 'rolling_std_7', 'rolling_mean_14', 'rolling_std_14', 'rolling_mean_28', 'rolling_std_28', 'rolling_mean_56', 'rolling_std_56', 'rolling_mean_168', 'rolling_std_168', 'rolling_mean_tmp_1_7', 'rolling_mean_tmp_1_14', 'rolling_mean_tmp_1_28', 'rolling_mean_tmp_1_56', 'rolling_mean_tmp_7_7', 'rolling_mean_tmp_7_14', 'rolling_mean_tmp_7_28', 'rolling_mean_tmp_7_56', 'rolling_mean_tmp_14_7', 'rolling_mean_tmp_14_14', 'rolling_mean_tmp_14_28', 'rolling_mean_tmp_14_56']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's rmse: 1.92046\n",
      "[200]\tvalid_0's rmse: 1.85308\n",
      "[300]\tvalid_0's rmse: 1.83401\n",
      "[400]\tvalid_0's rmse: 1.82419\n",
      "[500]\tvalid_0's rmse: 1.81684\n",
      "[600]\tvalid_0's rmse: 1.81026\n",
      "[700]\tvalid_0's rmse: 1.80453\n",
      "[800]\tvalid_0's rmse: 1.79942\n",
      "[900]\tvalid_0's rmse: 1.79486\n",
      "[1000]\tvalid_0's rmse: 1.79045\n",
      "[1100]\tvalid_0's rmse: 1.78576\n",
      "[1200]\tvalid_0's rmse: 1.78172\n",
      "[1300]\tvalid_0's rmse: 1.77735\n",
      "[1400]\tvalid_0's rmse: 1.77303\n",
      "Train FOODS_2\n",
      "['item_id', 'store_id', 'state_id', 'release', 'sell_price', 'price_max', 'price_min', 'price_std', 'price_mean', 'price_norm', 'price_rank_dept', 'price_nunique', 'item_nunique', 'price_momentum', 'price_momentum_m', 'price_momentum_y', 'temperature_high', 'temperature_con', 'rainfall_m', 'snow_m', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', 'snap_CA', 'snap_TX', 'snap_WI', 'is_first_half_month', 'event_bef_weekend', 'event_after_weekend', 'NBA', 'event_attention_after', 'event_attention_bef', 'event_attention_sum', 'tm_d', 'tm_w', 'tm_m', 'tm_q', 'tm_y', 'tm_wm', 'tm_dw', 'tm_w_end', 'enc_state_id_mean', 'enc_state_id_std', 'enc_store_id_mean', 'enc_store_id_std', 'enc_item_id_mean', 'enc_item_id_std', 'sales_lag_28', 'sales_lag_29', 'sales_lag_30', 'sales_lag_31', 'sales_lag_32', 'sales_lag_33', 'sales_lag_34', 'sales_lag_35', 'sales_lag_36', 'sales_lag_37', 'sales_lag_38', 'sales_lag_39', 'sales_lag_40', 'sales_lag_41', 'sales_lag_42', 'rolling_mean_7', 'rolling_std_7', 'rolling_mean_14', 'rolling_std_14', 'rolling_mean_28', 'rolling_std_28', 'rolling_mean_56', 'rolling_std_56', 'rolling_mean_168', 'rolling_std_168', 'rolling_mean_tmp_1_7', 'rolling_mean_tmp_1_14', 'rolling_mean_tmp_1_28', 'rolling_mean_tmp_1_56', 'rolling_mean_tmp_7_7', 'rolling_mean_tmp_7_14', 'rolling_mean_tmp_7_28', 'rolling_mean_tmp_7_56', 'rolling_mean_tmp_14_7', 'rolling_mean_tmp_14_14', 'rolling_mean_tmp_14_28', 'rolling_mean_tmp_14_56']\n",
      "[100]\tvalid_0's rmse: 1.84185\n",
      "[200]\tvalid_0's rmse: 1.75557\n",
      "[300]\tvalid_0's rmse: 1.72691\n",
      "[400]\tvalid_0's rmse: 1.7104\n",
      "[500]\tvalid_0's rmse: 1.69624\n",
      "[600]\tvalid_0's rmse: 1.68499\n",
      "[700]\tvalid_0's rmse: 1.67536\n",
      "[800]\tvalid_0's rmse: 1.66702\n",
      "[900]\tvalid_0's rmse: 1.65969\n",
      "[1000]\tvalid_0's rmse: 1.65224\n",
      "[1100]\tvalid_0's rmse: 1.64524\n",
      "[1200]\tvalid_0's rmse: 1.63813\n",
      "[1300]\tvalid_0's rmse: 1.63192\n",
      "[1400]\tvalid_0's rmse: 1.62638\n",
      "Train FOODS_3\n",
      "['item_id', 'store_id', 'state_id', 'release', 'sell_price', 'price_max', 'price_min', 'price_std', 'price_mean', 'price_norm', 'price_rank_dept', 'price_nunique', 'item_nunique', 'price_momentum', 'price_momentum_m', 'price_momentum_y', 'temperature_high', 'temperature_con', 'rainfall_m', 'snow_m', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', 'snap_CA', 'snap_TX', 'snap_WI', 'is_first_half_month', 'event_bef_weekend', 'event_after_weekend', 'NBA', 'event_attention_after', 'event_attention_bef', 'event_attention_sum', 'tm_d', 'tm_w', 'tm_m', 'tm_q', 'tm_y', 'tm_wm', 'tm_dw', 'tm_w_end', 'enc_state_id_mean', 'enc_state_id_std', 'enc_store_id_mean', 'enc_store_id_std', 'enc_item_id_mean', 'enc_item_id_std', 'sales_lag_28', 'sales_lag_29', 'sales_lag_30', 'sales_lag_31', 'sales_lag_32', 'sales_lag_33', 'sales_lag_34', 'sales_lag_35', 'sales_lag_36', 'sales_lag_37', 'sales_lag_38', 'sales_lag_39', 'sales_lag_40', 'sales_lag_41', 'sales_lag_42', 'rolling_mean_7', 'rolling_std_7', 'rolling_mean_14', 'rolling_std_14', 'rolling_mean_28', 'rolling_std_28', 'rolling_mean_56', 'rolling_std_56', 'rolling_mean_168', 'rolling_std_168', 'rolling_mean_tmp_1_7', 'rolling_mean_tmp_1_14', 'rolling_mean_tmp_1_28', 'rolling_mean_tmp_1_56', 'rolling_mean_tmp_7_7', 'rolling_mean_tmp_7_14', 'rolling_mean_tmp_7_28', 'rolling_mean_tmp_7_56', 'rolling_mean_tmp_14_7', 'rolling_mean_tmp_14_14', 'rolling_mean_tmp_14_28', 'rolling_mean_tmp_14_56']\n",
      "[100]\tvalid_0's rmse: 2.64702\n",
      "[200]\tvalid_0's rmse: 2.59805\n",
      "[300]\tvalid_0's rmse: 2.57662\n",
      "[400]\tvalid_0's rmse: 2.56077\n",
      "[500]\tvalid_0's rmse: 2.54816\n",
      "[600]\tvalid_0's rmse: 2.53657\n",
      "[700]\tvalid_0's rmse: 2.5266\n",
      "[800]\tvalid_0's rmse: 2.51738\n",
      "[900]\tvalid_0's rmse: 2.50848\n",
      "[1000]\tvalid_0's rmse: 2.49952\n",
      "[1100]\tvalid_0's rmse: 2.49218\n",
      "[1200]\tvalid_0's rmse: 2.48476\n",
      "[1300]\tvalid_0's rmse: 2.47777\n",
      "[1400]\tvalid_0's rmse: 2.47108\n"
     ]
    }
   ],
   "source": [
    "for dept_id in DEPT_IDS:\n",
    "    print('Train', dept_id)\n",
    "    \n",
    "    # Get grid for current store\n",
    "    grid_df, features_columns = get_data_by_dept(dept_id)\n",
    "    \n",
    "    print(features_columns)\n",
    "    # Masks for \n",
    "    # Train (All data less than 1913)\n",
    "    # \"Validation\" (Last 28 days - not real validation set)\n",
    "    # Test (All data greater than 1913 day, \n",
    "    #       with some gap for recursive features)\n",
    "    train_mask = grid_df['d']<=END_TRAIN\n",
    "    valid_mask = train_mask&(grid_df['d']>(END_TRAIN-P_HORIZON))\n",
    "    preds_mask = grid_df['d']>(END_TRAIN-100)\n",
    "    \n",
    "    # Apply masks and save lgb dataset as bin\n",
    "    # to reduce memory spikes during dtype convertations\n",
    "    # https://github.com/Microsoft/LightGBM/issues/1032\n",
    "    # \"To avoid any conversions, you should always use np.float32\"\n",
    "    # or save to bin before start training\n",
    "    # https://www.kaggle.com/c/talkingdata-adtracking-fraud-detection/discussion/53773\n",
    "    train_data = lgb.Dataset(grid_df[train_mask][features_columns], \n",
    "                       label=grid_df[train_mask][TARGET])\n",
    "    \n",
    "    \n",
    "    valid_data = lgb.Dataset(grid_df[valid_mask][features_columns], \n",
    "                       label=grid_df[valid_mask][TARGET])\n",
    "\n",
    "    \n",
    "    # Saving part of the dataset for later predictions\n",
    "    # Removing features that we need to calculate recursively \n",
    "    grid_df = grid_df[preds_mask].reset_index(drop=True)\n",
    "    keep_cols = [col for col in list(grid_df) if '_tmp_' not in col]\n",
    "    grid_df = grid_df[keep_cols]\n",
    "    grid_df.to_pickle('test_'+dept_id+'.pkl')\n",
    "    del grid_df\n",
    "    gc.collect()\n",
    "    \n",
    "    # Launch seeder again to make lgb training 100% deterministic\n",
    "    # with each \"code line\" np.random \"evolves\" \n",
    "    # so we need (may want) to \"reset\" it\n",
    "    seed_everything(SEED)\n",
    "    estimator = lgb.train(lgb_params,\n",
    "                          train_data,\n",
    "                          valid_sets = [valid_data],\n",
    "                          verbose_eval = 100,\n",
    "                          )\n",
    "    \n",
    "    \n",
    "    # Save model - it's not real '.bin' but a pickle file\n",
    "    # estimator = lgb.Booster(model_file='model.txt')\n",
    "    # can only predict with the best iteration (or the saving iteration)\n",
    "    # pickle.dump gives us more flexibility\n",
    "    # like estimator.predict(TEST, num_iteration=100)\n",
    "    # num_iteration - number of iteration want to predict with, \n",
    "    # NULL or <= 0 means use best iteration\n",
    "    model_name = 'lgb_model_'+dept_id+'_v'+str(VER)+'.bin'\n",
    "    pickle.dump(estimator, open(model_name, 'wb'))\n",
    "\n",
    "    # Remove temporary files and objects \n",
    "    # to free some hdd space and ram memory\n",
    "    #     !rm train_data.bin\n",
    "    del train_data, valid_data, estimator\n",
    "    gc.collect()\n",
    "    \n",
    "    # \"Keep\" models features for predictions\n",
    "    MODEL_FEATURES = features_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict | Day: 1\n",
      "##########  3.74 min round |  3.74 min total |  37316.21 day sales |\n",
      "Predict | Day: 2\n",
      "##########  3.75 min round |  7.49 min total |  35213.16 day sales |\n",
      "Predict | Day: 3\n",
      "##########  3.75 min round |  11.24 min total |  35067.32 day sales |\n",
      "Predict | Day: 4\n",
      "##########  3.74 min round |  14.98 min total |  35157.40 day sales |\n",
      "Predict | Day: 5\n",
      "##########  3.75 min round |  18.73 min total |  41784.54 day sales |\n",
      "Predict | Day: 6\n",
      "##########  3.78 min round |  22.51 min total |  50667.76 day sales |\n",
      "Predict | Day: 7\n",
      "##########  3.74 min round |  26.25 min total |  53627.43 day sales |\n",
      "Predict | Day: 8\n",
      "##########  3.75 min round |  30.00 min total |  43407.00 day sales |\n",
      "Predict | Day: 9\n",
      "##########  3.76 min round |  33.76 min total |  43563.08 day sales |\n",
      "Predict | Day: 10\n",
      "##########  3.76 min round |  37.53 min total |  38276.31 day sales |\n",
      "Predict | Day: 11\n",
      "##########  3.76 min round |  41.29 min total |  40718.75 day sales |\n",
      "Predict | Day: 12\n",
      "##########  3.76 min round |  45.05 min total |  45385.27 day sales |\n",
      "Predict | Day: 13\n",
      "##########  3.75 min round |  48.80 min total |  52885.19 day sales |\n",
      "Predict | Day: 14\n",
      "##########  3.79 min round |  52.59 min total |  46297.98 day sales |\n",
      "Predict | Day: 15\n",
      "##########  3.75 min round |  56.34 min total |  44982.76 day sales |\n",
      "Predict | Day: 16\n",
      "##########  3.73 min round |  60.07 min total |  39441.36 day sales |\n",
      "Predict | Day: 17\n",
      "##########  3.74 min round |  63.81 min total |  39795.61 day sales |\n",
      "Predict | Day: 18\n",
      "##########  3.74 min round |  67.55 min total |  40457.15 day sales |\n",
      "Predict | Day: 19\n",
      "##########  3.74 min round |  71.30 min total |  43584.01 day sales |\n",
      "Predict | Day: 20\n",
      "##########  3.74 min round |  75.04 min total |  53160.71 day sales |\n",
      "Predict | Day: 21\n",
      "##########  3.73 min round |  78.77 min total |  56488.30 day sales |\n",
      "Predict | Day: 22\n",
      "##########  3.74 min round |  82.51 min total |  41599.20 day sales |\n",
      "Predict | Day: 23\n",
      "##########  3.74 min round |  86.25 min total |  37606.78 day sales |\n",
      "Predict | Day: 24\n",
      "##########  3.74 min round |  90.00 min total |  36698.65 day sales |\n",
      "Predict | Day: 25\n",
      "##########  3.75 min round |  93.74 min total |  36795.72 day sales |\n",
      "Predict | Day: 26\n",
      "##########  3.75 min round |  97.49 min total |  41623.88 day sales |\n",
      "Predict | Day: 27\n",
      "##########  3.75 min round |  101.24 min total |  50056.05 day sales |\n",
      "Predict | Day: 28\n",
      "##########  3.76 min round |  105.00 min total |  50801.43 day sales |\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>F1</th>\n",
       "      <th>F2</th>\n",
       "      <th>F3</th>\n",
       "      <th>F4</th>\n",
       "      <th>F5</th>\n",
       "      <th>F6</th>\n",
       "      <th>F7</th>\n",
       "      <th>F8</th>\n",
       "      <th>F9</th>\n",
       "      <th>...</th>\n",
       "      <th>F19</th>\n",
       "      <th>F20</th>\n",
       "      <th>F21</th>\n",
       "      <th>F22</th>\n",
       "      <th>F23</th>\n",
       "      <th>F24</th>\n",
       "      <th>F25</th>\n",
       "      <th>F26</th>\n",
       "      <th>F27</th>\n",
       "      <th>F28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_001_CA_1_validation</td>\n",
       "      <td>0.798365</td>\n",
       "      <td>0.764451</td>\n",
       "      <td>0.724869</td>\n",
       "      <td>0.791378</td>\n",
       "      <td>0.933036</td>\n",
       "      <td>1.031330</td>\n",
       "      <td>0.945393</td>\n",
       "      <td>0.830390</td>\n",
       "      <td>0.836094</td>\n",
       "      <td>...</td>\n",
       "      <td>0.771973</td>\n",
       "      <td>1.081486</td>\n",
       "      <td>1.140960</td>\n",
       "      <td>0.826052</td>\n",
       "      <td>0.759391</td>\n",
       "      <td>0.748489</td>\n",
       "      <td>0.712071</td>\n",
       "      <td>0.788055</td>\n",
       "      <td>1.007608</td>\n",
       "      <td>1.023545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_002_CA_1_validation</td>\n",
       "      <td>0.229536</td>\n",
       "      <td>0.192468</td>\n",
       "      <td>0.151444</td>\n",
       "      <td>0.182707</td>\n",
       "      <td>0.228488</td>\n",
       "      <td>0.252865</td>\n",
       "      <td>0.274073</td>\n",
       "      <td>0.270748</td>\n",
       "      <td>0.230314</td>\n",
       "      <td>...</td>\n",
       "      <td>0.183348</td>\n",
       "      <td>0.218204</td>\n",
       "      <td>0.223137</td>\n",
       "      <td>0.200566</td>\n",
       "      <td>0.192202</td>\n",
       "      <td>0.231544</td>\n",
       "      <td>0.203672</td>\n",
       "      <td>0.224412</td>\n",
       "      <td>0.286147</td>\n",
       "      <td>0.302958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOBBIES_1_003_CA_1_validation</td>\n",
       "      <td>0.444305</td>\n",
       "      <td>0.455331</td>\n",
       "      <td>0.498127</td>\n",
       "      <td>0.476141</td>\n",
       "      <td>0.620738</td>\n",
       "      <td>0.758979</td>\n",
       "      <td>0.575984</td>\n",
       "      <td>0.424609</td>\n",
       "      <td>0.440465</td>\n",
       "      <td>...</td>\n",
       "      <td>0.506963</td>\n",
       "      <td>0.703678</td>\n",
       "      <td>0.663228</td>\n",
       "      <td>0.449868</td>\n",
       "      <td>0.434801</td>\n",
       "      <td>0.432434</td>\n",
       "      <td>0.429254</td>\n",
       "      <td>0.513214</td>\n",
       "      <td>0.728716</td>\n",
       "      <td>0.714833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOBBIES_1_004_CA_1_validation</td>\n",
       "      <td>1.617201</td>\n",
       "      <td>1.346853</td>\n",
       "      <td>1.309672</td>\n",
       "      <td>1.450811</td>\n",
       "      <td>1.926036</td>\n",
       "      <td>2.366938</td>\n",
       "      <td>2.937266</td>\n",
       "      <td>1.733563</td>\n",
       "      <td>1.454747</td>\n",
       "      <td>...</td>\n",
       "      <td>2.008037</td>\n",
       "      <td>2.575520</td>\n",
       "      <td>3.290043</td>\n",
       "      <td>1.704270</td>\n",
       "      <td>1.489606</td>\n",
       "      <td>1.308871</td>\n",
       "      <td>1.299560</td>\n",
       "      <td>1.692514</td>\n",
       "      <td>2.925206</td>\n",
       "      <td>3.008933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOBBIES_1_005_CA_1_validation</td>\n",
       "      <td>0.988069</td>\n",
       "      <td>0.928129</td>\n",
       "      <td>0.984490</td>\n",
       "      <td>1.048214</td>\n",
       "      <td>1.226376</td>\n",
       "      <td>1.363948</td>\n",
       "      <td>1.377518</td>\n",
       "      <td>0.969183</td>\n",
       "      <td>1.048397</td>\n",
       "      <td>...</td>\n",
       "      <td>1.123732</td>\n",
       "      <td>1.458751</td>\n",
       "      <td>1.487868</td>\n",
       "      <td>0.974727</td>\n",
       "      <td>0.922636</td>\n",
       "      <td>0.940651</td>\n",
       "      <td>0.983265</td>\n",
       "      <td>1.106530</td>\n",
       "      <td>1.441950</td>\n",
       "      <td>1.376951</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id        F1        F2        F3        F4  \\\n",
       "0  HOBBIES_1_001_CA_1_validation  0.798365  0.764451  0.724869  0.791378   \n",
       "1  HOBBIES_1_002_CA_1_validation  0.229536  0.192468  0.151444  0.182707   \n",
       "2  HOBBIES_1_003_CA_1_validation  0.444305  0.455331  0.498127  0.476141   \n",
       "3  HOBBIES_1_004_CA_1_validation  1.617201  1.346853  1.309672  1.450811   \n",
       "4  HOBBIES_1_005_CA_1_validation  0.988069  0.928129  0.984490  1.048214   \n",
       "\n",
       "         F5        F6        F7        F8        F9  ...       F19       F20  \\\n",
       "0  0.933036  1.031330  0.945393  0.830390  0.836094  ...  0.771973  1.081486   \n",
       "1  0.228488  0.252865  0.274073  0.270748  0.230314  ...  0.183348  0.218204   \n",
       "2  0.620738  0.758979  0.575984  0.424609  0.440465  ...  0.506963  0.703678   \n",
       "3  1.926036  2.366938  2.937266  1.733563  1.454747  ...  2.008037  2.575520   \n",
       "4  1.226376  1.363948  1.377518  0.969183  1.048397  ...  1.123732  1.458751   \n",
       "\n",
       "        F21       F22       F23       F24       F25       F26       F27  \\\n",
       "0  1.140960  0.826052  0.759391  0.748489  0.712071  0.788055  1.007608   \n",
       "1  0.223137  0.200566  0.192202  0.231544  0.203672  0.224412  0.286147   \n",
       "2  0.663228  0.449868  0.434801  0.432434  0.429254  0.513214  0.728716   \n",
       "3  3.290043  1.704270  1.489606  1.308871  1.299560  1.692514  2.925206   \n",
       "4  1.487868  0.974727  0.922636  0.940651  0.983265  1.106530  1.441950   \n",
       "\n",
       "        F28  \n",
       "0  1.023545  \n",
       "1  0.302958  \n",
       "2  0.714833  \n",
       "3  3.008933  \n",
       "4  1.376951  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Dummy DataFrame to store predictions\n",
    "all_preds = pd.DataFrame()\n",
    "\n",
    "# Join back the Test dataset with \n",
    "# a small part of the training data \n",
    "# to make recursive features\n",
    "base_test = get_base_test()\n",
    "\n",
    "# Timer to measure predictions time \n",
    "main_time = time.time()\n",
    "\n",
    "# Loop over each prediction day\n",
    "# As rolling lags are the most timeconsuming\n",
    "# we will calculate it for whole day\n",
    "\n",
    "\n",
    "for PREDICT_DAY in range(1,29):    \n",
    "    print('Predict | Day:', PREDICT_DAY)\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Make temporary grid to calculate rolling lags\n",
    "    grid_df = base_test.copy()\n",
    "    \n",
    "    \n",
    "    lag_df_new = pd.DataFrame()\n",
    "\n",
    "    lag_df_new=make_lag_roll(ROLS_SPLIT,lag_df_new)\n",
    "\n",
    "\n",
    "    grid_df =  grid_df.merge(lag_df_new, on=['id','d'], how='left')\n",
    "\n",
    "\n",
    "    for dept_id in DEPT_IDS:\n",
    "\n",
    "        # Read all our models and make predictions\n",
    "        # for each day/store pairs\n",
    "        model_path = 'lgb_model_'+dept_id+'_v'+str(VER)+'.bin' \n",
    "\n",
    "        estimator = pickle.load(open(model_path, 'rb'))\n",
    "\n",
    "        day_mask = base_test['d']==(END_TRAIN+PREDICT_DAY)\n",
    "        dept_mask = base_test['dept_id']==dept_id\n",
    "\n",
    "        mask = (day_mask)&(dept_mask)\n",
    "        base_test[TARGET][mask] = estimator.predict(grid_df[mask][MODEL_FEATURES])\n",
    "\n",
    "    # Make good column naming and add \n",
    "    # to all_preds DataFrame\n",
    "    temp_df = base_test[day_mask][['id',TARGET]]\n",
    "    temp_df.columns = ['id','F'+str(PREDICT_DAY)]\n",
    "    if 'id' in list(all_preds):\n",
    "        all_preds = all_preds.merge(temp_df, on=['id'], how='left')\n",
    "    else:\n",
    "        all_preds = temp_df.copy()\n",
    "\n",
    "    print('#'*10, ' %0.2f min round |' % ((time.time() - start_time) / 60),\n",
    "                  ' %0.2f min total |' % ((time.time() - main_time) / 60),\n",
    "                  ' %0.2f day sales |' % (temp_df['F'+str(PREDICT_DAY)].sum()))\n",
    "    \n",
    "    del temp_df, lag_df_new\n",
    "\n",
    "all_preds = all_preds.reset_index(drop=True)\n",
    "all_preds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading competition sample submission and\n",
    "# merging our predictions\n",
    "# As we have predictions only for \"_validation\" data\n",
    "# we need to do fillna() for \"_evaluation\" items\n",
    "submission = pd.read_csv('sample_submission.csv')[['id']]\n",
    "submission = submission.merge(all_preds, on=['id'], how='left').fillna(0)\n",
    "submission.to_csv('lgb_bydept_fe6.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
