{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys, gc, time, warnings, pickle, psutil, random\n",
    "from sklearn.model_selection import GroupKFold\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# :seed to make all processes deterministic     # type: int\n",
    "def seed_everything(seed=0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "def get_data_by_store(store_id):\n",
    "    \n",
    "    # Read and contact basic feature\n",
    "    df = pd.concat([pd.read_pickle(BASE),\n",
    "                    pd.read_pickle(PRICE).iloc[:,2:],\n",
    "                    pd.read_pickle(CALENDAR).iloc[:,2:]],\n",
    "                    axis=1)\n",
    "    \n",
    "    # Leave only relevant store\n",
    "    df = df[df['store_id']==store_id]\n",
    "\n",
    "    # With memory limits we have to read \n",
    "    # lags and mean encoding features\n",
    "    # separately and drop items that we don't need.\n",
    "    # As our Features Grids are aligned \n",
    "    # we can use index to keep only necessary rows\n",
    "    # Alignment is good for us as concat uses less memory than merge.\n",
    "    df2 = pd.read_pickle(MEAN_ENC)[mean_features]\n",
    "    df2 = df2[df2.index.isin(df.index)]\n",
    "    \n",
    "    df3 = pd.read_pickle(LAGS).iloc[:,3:]\n",
    "    df3 = df3[df3.index.isin(df.index)]\n",
    "    \n",
    "    df = pd.concat([df, df2], axis=1)\n",
    "    del df2 # to not reach memory limit \n",
    "    \n",
    "    df = pd.concat([df, df3], axis=1)\n",
    "    del df3 # to not reach memory limit \n",
    "    \n",
    "    if store_id in ['CA_1', 'CA_2', 'CA_3','CA_4','TX_1','TX_2','TX_3']:\n",
    "        remove_features = ['id','state_id','store_id','date','wm_yr_wk','d',TARGET,'cluster','snow_m']\n",
    "    else:\n",
    "        remove_features = ['id','state_id','store_id','date','wm_yr_wk','d',TARGET,'cluster']\n",
    "    \n",
    "    # Create features list\n",
    "    features = [col for col in list(df) if col not in remove_features]\n",
    "    df = df[['id','d',TARGET]+features]\n",
    "    \n",
    "    # Skipping first n rows\n",
    "    df = df[df['d']>=START_TRAIN].reset_index(drop=True)\n",
    "    \n",
    "    return df, features\n",
    "\n",
    "# Recombine Test set after training\n",
    "def get_base_test():\n",
    "    base_test = pd.DataFrame()\n",
    "\n",
    "    for store_id in STORES_IDS:\n",
    "        temp_df = pd.read_pickle('test_'+store_id+str(VER)+'.pkl')\n",
    "        temp_df['store_id'] = store_id\n",
    "        base_test = pd.concat([base_test, temp_df]).reset_index(drop=True)\n",
    "    \n",
    "    return base_test\n",
    "\n",
    "\n",
    "########################### Helper to make dynamic rolling lags\n",
    "#################################################################################\n",
    "def make_lag(LAG_DAY):\n",
    "    lag_df = base_test[['id','d',TARGET]]\n",
    "    col_name = 'sales_lag_'+str(LAG_DAY)\n",
    "    lag_df[col_name] = lag_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(LAG_DAY)).astype(np.float16)\n",
    "    return lag_df[[col_name]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lag_roll(LAG_DAY,lag_df_new):\n",
    "   \n",
    "    lag_df = base_test[['id','d',TARGET]]\n",
    "    \n",
    "    lag_df=lag_df.sort_values(by=[\"d\"])\n",
    "  \n",
    "    for i in range(0,len(LAG_DAY)):\n",
    "\n",
    "        shift_day = LAG_DAY[i][0]\n",
    "        roll_wind = LAG_DAY[i][1]\n",
    "        col_name = 'rolling_mean_tmp_'+str(shift_day)+'_'+str(roll_wind)\n",
    "        lag_df[col_name] = (lag_df.groupby(['id'])[TARGET]).transform(lambda x: x.shift(shift_day).rolling(roll_wind).mean())\n",
    "    lag_df_new=lag_df.drop(columns=[\"sales\"])\n",
    "    return lag_df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "lgb_params = {\n",
    "                    'boosting_type': 'gbdt',\n",
    "                    'objective': 'tweedie',\n",
    "                    'tweedie_variance_power': 1.1,\n",
    "                    'metric': 'rmse',\n",
    "                    'subsample': 0.5,\n",
    "                    'subsample_freq': 1,\n",
    "                    'learning_rate': 0.03,\n",
    "                    \"lambda\":0.1,\n",
    "                    'num_leaves': 2**11-1,\n",
    "                    'min_data_in_leaf': 2**12-1,\n",
    "                    'feature_fraction': 0.5,\n",
    "                    'max_bin': 100,\n",
    "                    'n_estimators': 1400,\n",
    "                    'boost_from_average': False,\n",
    "                    'verbose': -1,\n",
    "                } \n",
    "\n",
    "\n",
    "\n",
    "# lgb_params ={\n",
    "#         \"objective\" : \"tweedie\",\n",
    "#         \"metric\" :\"rmse\",\n",
    "#         \"force_row_wise\" : True,\n",
    "#         \"learning_rate\" : 0.075,\n",
    "#         \"sub_feature\" : 0.8,\n",
    "#         \"sub_row\" : 0.75,\n",
    "#         \"bagging_freq\" : 1,\n",
    "#         \"lambda_l2\" : 0.1,\n",
    "#         \"metric\": [\"rmse\"],\n",
    "#         \"nthread\": -1,\n",
    "#         \"tweedie_variance_power\":1.1,\n",
    "#     'verbosity': 1,\n",
    "# #     'num_iterations' : 1500,\n",
    "#     'num_leaves': 128,\n",
    "#     \"min_data_in_leaf\": 104,\n",
    "#     }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Let's look closer on params\n",
    "\n",
    "## 'boosting_type': 'gbdt'\n",
    "# we have 'goss' option for faster training\n",
    "# but it normally leads to underfit.\n",
    "# Also there is good 'dart' mode\n",
    "# but it takes forever to train\n",
    "# and model performance depends \n",
    "# a lot on random factor \n",
    "# https://www.kaggle.com/c/home-credit-default-risk/discussion/60921\n",
    "\n",
    "## 'objective': 'tweedie'\n",
    "# Tweedie Gradient Boosting for Extremely\n",
    "# Unbalanced Zero-inflated Data\n",
    "# https://arxiv.org/pdf/1811.10192.pdf\n",
    "# and many more articles about tweediie\n",
    "#\n",
    "# Strange (for me) but Tweedie is close in results\n",
    "# to my own ugly loss.\n",
    "# My advice here - make OWN LOSS function\n",
    "# https://www.kaggle.com/c/m5-forecasting-accuracy/discussion/140564\n",
    "# https://www.kaggle.com/c/m5-forecasting-accuracy/discussion/143070\n",
    "# I think many of you already using it (after poisson kernel appeared) \n",
    "# (kagglers are very good with \"params\" testing and tuning).\n",
    "# Try to figure out why Tweedie works.\n",
    "# probably it will show you new features options\n",
    "# or data transformation (Target transformation?).\n",
    "\n",
    "## 'tweedie_variance_power': 1.1\n",
    "# default = 1.5\n",
    "# set this closer to 2 to shift towards a Gamma distribution\n",
    "# set this closer to 1 to shift towards a Poisson distribution\n",
    "# my CV shows 1.1 is optimal \n",
    "# but you can make your own choice\n",
    "\n",
    "## 'metric': 'rmse'\n",
    "# Doesn't mean anything to us\n",
    "# as competition metric is different\n",
    "# and we don't use early stoppings here.\n",
    "# So rmse serves just for general \n",
    "# model performance overview.\n",
    "# Also we use \"fake\" validation set\n",
    "# (as it makes part of the training set)\n",
    "# so even general rmse score doesn't mean anything))\n",
    "# https://www.kaggle.com/c/m5-forecasting-accuracy/discussion/133834\n",
    "\n",
    "## 'subsample': 0.5\n",
    "# Serves to fight with overfit\n",
    "# this will randomly select part of data without resampling\n",
    "# Chosen by CV (my CV can be wrong!)\n",
    "# Next kernel will be about CV\n",
    "\n",
    "##'subsample_freq': 1\n",
    "# frequency for bagging\n",
    "# default value - seems ok\n",
    "\n",
    "## 'learning_rate': 0.03\n",
    "# Chosen by CV\n",
    "# Smaller - longer training\n",
    "# but there is an option to stop \n",
    "# in \"local minimum\"\n",
    "# Bigger - faster training\n",
    "# but there is a chance to\n",
    "# not find \"global minimum\" minimum\n",
    "\n",
    "## 'num_leaves': 2**11-1\n",
    "## 'min_data_in_leaf': 2**12-1\n",
    "# Force model to use more features\n",
    "# We need it to reduce \"recursive\"\n",
    "# error impact.\n",
    "# Also it leads to overfit\n",
    "# that's why we use small \n",
    "\n",
    "# 'max_bin': 100\n",
    "## l1, l2 regularizations\n",
    "# https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c\n",
    "# Good tiny explanation\n",
    "# l2 can work with bigger num_leaves\n",
    "# but my CV doesn't show boost\n",
    "                    \n",
    "## 'n_estimators': 1400\n",
    "# CV shows that there should be\n",
    "# different values for each state/store.\n",
    "# Current value was chosen \n",
    "# for general purpose.\n",
    "# As we don't use any early stopings\n",
    "# careful to not overfit Public LB.\n",
    "\n",
    "##'feature_fraction': 0.5\n",
    "# LightGBM will randomly select \n",
    "# part of features on each iteration (tree).\n",
    "# We have maaaany features\n",
    "# and many of them are \"duplicates\"\n",
    "# and many just \"noise\"\n",
    "# good values here - 0.5-0.7 (by CV)\n",
    "\n",
    "## 'boost_from_average': False\n",
    "# There is some \"problem\"\n",
    "# to code boost_from_average for \n",
    "# custom loss\n",
    "# 'True' makes training faster\n",
    "# BUT carefull use it\n",
    "# https://github.com/microsoft/LightGBM/issues/1514"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "VER = 8                       # Our model version\n",
    "SEED = 42                      # We want all things\n",
    "seed_everything(SEED)            # to be as deterministic \n",
    "lgb_params['seed'] = SEED        # as possible\n",
    "N_CORES = psutil.cpu_count()     # Available CPU cores\n",
    "\n",
    "\n",
    "#LIMITS and const\n",
    "TARGET      = 'sales'            # Our target\n",
    "START_TRAIN = 0                  # We can skip some rows (Nans/faster training)\n",
    "END_TRAIN   = 1941               # End day of our train set, change this part for final\n",
    "P_HORIZON   = 28                 # Prediction horizon\n",
    "\n",
    "#FEATURES to remove\n",
    "## These features lead to overfit\n",
    "## or values not present in test set\n",
    "mean_features   = ['enc_cat_id_mean','enc_cat_id_std',\n",
    "                   'enc_dept_id_mean','enc_dept_id_std',\n",
    "                   'enc_item_id_mean','enc_item_id_std'] \n",
    "\n",
    "#PATHS for Features\n",
    "BASE     = 'grid_part_1.pkl'\n",
    "PRICE    = 'grid_part_2.pkl'\n",
    "CALENDAR = 'grid_part_3.pkl'\n",
    "LAGS     = 'lags_df_28_v3.pkl'\n",
    "MEAN_ENC = 'mean_encoding_df.pkl'\n",
    "\n",
    "\n",
    "# AUX(pretrained) Models paths\n",
    "\n",
    "#STORES ids\n",
    "STORES_IDS = ['CA_1']\n",
    "\n",
    "#SPLITS for lags creation\n",
    "SHIFT_DAY  = 28\n",
    "N_LAGS     = 15\n",
    "LAGS_SPLIT = [col for col in range(SHIFT_DAY,SHIFT_DAY+N_LAGS)]\n",
    "ROLS_SPLIT = []\n",
    "for i in [1,7,14]:\n",
    "    for j in [7,14,28,56]:\n",
    "        ROLS_SPLIT.append([i,j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train CA_1\n",
      "['item_id', 'dept_id', 'cat_id', 'release', 'sell_price', 'price_max', 'price_min', 'price_std', 'price_mean', 'price_norm', 'price_rank_dept', 'price_nunique', 'item_nunique', 'price_momentum', 'price_momentum_m', 'price_momentum_y', 'temperature_high', 'temperature_con', 'rainfall_m', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', 'snap_CA', 'snap_TX', 'snap_WI', 'is_first_half_month', 'event_bef_weekend', 'event_after_weekend', 'NBA', 'event_attention_after', 'event_attention_bef', 'event_attention_sum', 'tm_d', 'tm_w', 'tm_m', 'tm_q', 'tm_y', 'tm_wm', 'tm_dw', 'tm_w_end', 'enc_cat_id_mean', 'enc_cat_id_std', 'enc_dept_id_mean', 'enc_dept_id_std', 'enc_item_id_mean', 'enc_item_id_std', 'sales_lag_28', 'sales_lag_29', 'sales_lag_30', 'sales_lag_31', 'sales_lag_32', 'sales_lag_33', 'sales_lag_34', 'sales_lag_35', 'sales_lag_36', 'sales_lag_37', 'sales_lag_38', 'sales_lag_39', 'sales_lag_40', 'sales_lag_41', 'sales_lag_42', 'rolling_mean_7', 'rolling_std_7', 'rolling_mean_14', 'rolling_std_14', 'rolling_mean_28', 'rolling_std_28', 'rolling_mean_56', 'rolling_std_56', 'rolling_mean_168', 'rolling_std_168', 'rolling_mean_tmp_1_7', 'rolling_mean_tmp_1_14', 'rolling_mean_tmp_1_28', 'rolling_mean_tmp_1_56', 'rolling_mean_tmp_7_7', 'rolling_mean_tmp_7_14', 'rolling_mean_tmp_7_28', 'rolling_mean_tmp_7_56', 'rolling_mean_tmp_14_7', 'rolling_mean_tmp_14_14', 'rolling_mean_tmp_14_28', 'rolling_mean_tmp_14_56', 'rolling_quantile_97_28', 'rolling_quantile_87.5_28', 'rolling_quantile_50_28', 'rolling_quantile_22.5_28', 'rolling_quantile_3_28', 'rolling_quantile_97_56', 'rolling_quantile_87.5_56', 'rolling_quantile_50_56', 'rolling_quantile_22.5_56', 'rolling_quantile_3_56', 'rolling_quantile_97_168', 'rolling_quantile_87.5_168', 'rolling_quantile_50_168', 'rolling_quantile_22.5_168', 'rolling_quantile_3_168']\n",
      "Fold: 0\n",
      "3182341 1593998\n",
      "[100]\ttraining's rmse: 2.59652\tvalid_1's rmse: 2.60736\n",
      "[200]\ttraining's rmse: 2.48956\tvalid_1's rmse: 2.52767\n",
      "[300]\ttraining's rmse: 2.45261\tvalid_1's rmse: 2.50709\n",
      "[400]\ttraining's rmse: 2.42615\tvalid_1's rmse: 2.4968\n",
      "[500]\ttraining's rmse: 2.40474\tvalid_1's rmse: 2.48969\n",
      "[600]\ttraining's rmse: 2.38757\tvalid_1's rmse: 2.48574\n",
      "[700]\ttraining's rmse: 2.37174\tvalid_1's rmse: 2.48286\n",
      "[800]\ttraining's rmse: 2.35623\tvalid_1's rmse: 2.47993\n",
      "[900]\ttraining's rmse: 2.34224\tvalid_1's rmse: 2.47802\n",
      "[1000]\ttraining's rmse: 2.3299\tvalid_1's rmse: 2.4761\n",
      "[1100]\ttraining's rmse: 2.3184\tvalid_1's rmse: 2.47514\n",
      "[1200]\ttraining's rmse: 2.30726\tvalid_1's rmse: 2.47321\n",
      "[1300]\ttraining's rmse: 2.29782\tvalid_1's rmse: 2.47199\n",
      "[1400]\ttraining's rmse: 2.28787\tvalid_1's rmse: 2.47034\n",
      "Fold: 1\n",
      "3188366 1587973\n",
      "[100]\ttraining's rmse: 2.57888\tvalid_1's rmse: 2.62371\n",
      "[200]\ttraining's rmse: 2.47356\tvalid_1's rmse: 2.54164\n",
      "[300]\ttraining's rmse: 2.4386\tvalid_1's rmse: 2.5186\n",
      "[400]\ttraining's rmse: 2.41325\tvalid_1's rmse: 2.50456\n",
      "[500]\ttraining's rmse: 2.39167\tvalid_1's rmse: 2.49609\n",
      "[600]\ttraining's rmse: 2.37341\tvalid_1's rmse: 2.4901\n",
      "[700]\ttraining's rmse: 2.35721\tvalid_1's rmse: 2.4857\n",
      "[800]\ttraining's rmse: 2.3414\tvalid_1's rmse: 2.48117\n",
      "[900]\ttraining's rmse: 2.32698\tvalid_1's rmse: 2.47846\n",
      "[1000]\ttraining's rmse: 2.31534\tvalid_1's rmse: 2.47643\n",
      "[1100]\ttraining's rmse: 2.30528\tvalid_1's rmse: 2.4747\n",
      "[1200]\ttraining's rmse: 2.2957\tvalid_1's rmse: 2.47364\n",
      "[1300]\ttraining's rmse: 2.28622\tvalid_1's rmse: 2.47263\n",
      "[1400]\ttraining's rmse: 2.27764\tvalid_1's rmse: 2.47158\n",
      "Fold: 2\n",
      "3181971 1594368\n",
      "[100]\ttraining's rmse: 2.60219\tvalid_1's rmse: 2.59242\n",
      "[200]\ttraining's rmse: 2.50149\tvalid_1's rmse: 2.49677\n",
      "[300]\ttraining's rmse: 2.46477\tvalid_1's rmse: 2.46989\n",
      "[400]\ttraining's rmse: 2.44019\tvalid_1's rmse: 2.45512\n",
      "[500]\ttraining's rmse: 2.42006\tvalid_1's rmse: 2.4472\n",
      "[600]\ttraining's rmse: 2.40415\tvalid_1's rmse: 2.44114\n",
      "[700]\ttraining's rmse: 2.38947\tvalid_1's rmse: 2.43749\n",
      "[800]\ttraining's rmse: 2.37693\tvalid_1's rmse: 2.43453\n",
      "[900]\ttraining's rmse: 2.36481\tvalid_1's rmse: 2.43286\n",
      "[1000]\ttraining's rmse: 2.35392\tvalid_1's rmse: 2.43004\n",
      "[1100]\ttraining's rmse: 2.34412\tvalid_1's rmse: 2.42876\n",
      "[1200]\ttraining's rmse: 2.33417\tvalid_1's rmse: 2.42634\n",
      "[1300]\ttraining's rmse: 2.32526\tvalid_1's rmse: 2.42465\n",
      "[1400]\ttraining's rmse: 2.31666\tvalid_1's rmse: 2.42376\n"
     ]
    }
   ],
   "source": [
    "for store_id in STORES_IDS:\n",
    "    print('Train', store_id)\n",
    "    \n",
    "    # Get grid for current store\n",
    "    grid_df, features_columns = get_data_by_store(store_id)\n",
    "    print(features_columns)\n",
    "    \n",
    "    # Masks for \n",
    "    # Train (All data less than 1913)\n",
    "    # \"Validation\" (Last 28 days - not real validatio set)\n",
    "    # Test (All data greater than 1913 day, \n",
    "    #       with some gap for recursive features)\n",
    "    train_mask = grid_df['d']<=END_TRAIN\n",
    "    preds_mask = grid_df['d']>(END_TRAIN-100)\n",
    "    \n",
    "    ## Initiating our GroupKFold\n",
    "    folds = GroupKFold(n_splits=3)\n",
    "\n",
    "    # get subgroups for each week, year pair\n",
    "    grid_df['groups'] = grid_df['tm_w'].astype(str) + '_' + grid_df['tm_y'].astype(str)\n",
    "    split_groups = grid_df[train_mask]['groups']\n",
    "\n",
    "    # Main Data\n",
    "    X,y = grid_df[train_mask][features_columns], grid_df[train_mask][TARGET]\n",
    "        \n",
    "    # Saving part of the dataset for later predictions\n",
    "    # Removing features that we need to calculate recursively \n",
    "    grid_df = grid_df[preds_mask].reset_index(drop=True)\n",
    "    keep_cols = [col for col in list(grid_df) if '_tmp_' not in col]\n",
    "    grid_df = grid_df[keep_cols]\n",
    "    grid_df.to_pickle('test_'+store_id+str(VER)+'.pkl')\n",
    "    del grid_df\n",
    "    gc.collect()\n",
    "    \n",
    "    # Launch seeder again to make lgb training 100% deterministic\n",
    "    # with each \"code line\" np.random \"evolves\" \n",
    "    # so we need (may want) to \"reset\" it\n",
    "    \n",
    "    for fold_, (trn_idx, val_idx) in enumerate(folds.split(X, y, groups=split_groups)):\n",
    "        print('Fold:',fold_)\n",
    "        print(len(trn_idx),len(val_idx))\n",
    "        tr_x, tr_y = X.iloc[trn_idx,:], y[trn_idx]\n",
    "        v_X, v_y   = X.iloc[val_idx,:], y[val_idx] \n",
    "        train_data = lgb.Dataset(tr_x, label=tr_y)\n",
    "        valid_data = lgb.Dataset(v_X, label=v_y)  \n",
    "\n",
    "        seed_everything(SEED)\n",
    "        estimator = lgb.train(\n",
    "                lgb_params,\n",
    "                train_data,\n",
    "                valid_sets = [train_data, valid_data],\n",
    "                verbose_eval = 100,\n",
    "            )\n",
    "        # Save model - it's not real '.bin' but a pickle file\n",
    "        # estimator = lgb.Booster(model_file='model.txt')\n",
    "        # can only predict with the best iteration (or the saving iteration)\n",
    "        # pickle.dump gives us more flexibility\n",
    "        # like estimator.predict(TEST, num_iteration=100)\n",
    "        # num_iteration - number of iteration want to predict with, \n",
    "        # NULL or <= 0 means use best iteration\n",
    "        model_name = 'lgb_model_'+store_id+'_'+str(fold_)+'.bin'\n",
    "        pickle.dump(estimator, open(model_name, 'wb'))\n",
    "\n",
    "        # Remove temporary files and objects \n",
    "        # to free some hdd space and ram memory\n",
    "        del train_data, valid_data, estimator\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FOLDS\n",
    "CV_FOLDS = [0,1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 0\n",
      "Predict | Day: 1\n",
      "##########  8.13 min round |  8.13 min total |  4464.23 day sales |\n",
      "Predict | Day: 2\n",
      "##########  7.49 min round |  15.62 min total |  3951.86 day sales |\n",
      "Predict | Day: 3\n",
      "##########  6.95 min round |  22.57 min total |  3882.12 day sales |\n",
      "Predict | Day: 4\n",
      "##########  7.69 min round |  30.26 min total |  3904.06 day sales |\n",
      "Predict | Day: 5\n",
      "##########  7.65 min round |  37.91 min total |  4665.55 day sales |\n",
      "Predict | Day: 6\n",
      "##########  7.29 min round |  45.20 min total |  5794.91 day sales |\n",
      "Predict | Day: 7\n",
      "##########  7.23 min round |  52.44 min total |  6097.64 day sales |\n",
      "Predict | Day: 8\n",
      "##########  7.67 min round |  60.10 min total |  5355.73 day sales |\n",
      "Predict | Day: 9\n",
      "##########  7.68 min round |  67.78 min total |  4079.63 day sales |\n",
      "Predict | Day: 10\n",
      "##########  7.54 min round |  75.33 min total |  4635.82 day sales |\n",
      "Predict | Day: 11\n",
      "##########  6.96 min round |  82.28 min total |  4007.66 day sales |\n",
      "Predict | Day: 12\n",
      "##########  7.66 min round |  89.95 min total |  5564.31 day sales |\n",
      "Predict | Day: 13\n",
      "##########  7.66 min round |  97.61 min total |  6508.44 day sales |\n",
      "Predict | Day: 14\n",
      "##########  7.71 min round |  105.31 min total |  6683.37 day sales |\n",
      "Predict | Day: 15\n",
      "##########  6.85 min round |  112.16 min total |  5319.63 day sales |\n",
      "Predict | Day: 16\n",
      "##########  7.64 min round |  119.80 min total |  4692.83 day sales |\n",
      "Predict | Day: 17\n",
      "##########  7.65 min round |  127.46 min total |  4525.02 day sales |\n",
      "Predict | Day: 18\n",
      "##########  7.66 min round |  135.12 min total |  4628.75 day sales |\n",
      "Predict | Day: 19\n",
      "##########  6.88 min round |  141.99 min total |  5350.10 day sales |\n",
      "Predict | Day: 20\n",
      "##########  7.64 min round |  149.64 min total |  6354.12 day sales |\n",
      "Predict | Day: 21\n",
      "##########  7.66 min round |  157.30 min total |  6826.28 day sales |\n",
      "Predict | Day: 22\n",
      "##########  7.66 min round |  164.96 min total |  4935.44 day sales |\n",
      "Predict | Day: 23\n",
      "##########  6.85 min round |  171.81 min total |  4340.18 day sales |\n",
      "Predict | Day: 24\n",
      "##########  7.68 min round |  179.49 min total |  4305.08 day sales |\n",
      "Predict | Day: 25\n",
      "##########  7.71 min round |  187.19 min total |  4373.51 day sales |\n",
      "Predict | Day: 26\n",
      "##########  7.70 min round |  194.90 min total |  5200.14 day sales |\n",
      "Predict | Day: 27\n",
      "##########  6.85 min round |  201.75 min total |  6278.61 day sales |\n",
      "Predict | Day: 28\n",
      "##########  7.69 min round |  209.44 min total |  5959.72 day sales |\n",
      "FOLD: 1\n",
      "Predict | Day: 1\n",
      "##########  7.71 min round |  7.71 min total |  4463.35 day sales |\n",
      "Predict | Day: 2\n",
      "##########  7.72 min round |  15.43 min total |  3976.73 day sales |\n",
      "Predict | Day: 3\n",
      "##########  5.26 min round |  20.69 min total |  3880.43 day sales |\n",
      "Predict | Day: 4\n",
      "##########  4.35 min round |  25.04 min total |  3929.57 day sales |\n",
      "Predict | Day: 5\n",
      "##########  4.36 min round |  29.40 min total |  4757.20 day sales |\n",
      "Predict | Day: 6\n",
      "##########  4.36 min round |  33.76 min total |  5941.46 day sales |\n",
      "Predict | Day: 7\n",
      "##########  4.35 min round |  38.11 min total |  6239.53 day sales |\n",
      "Predict | Day: 8\n",
      "##########  4.35 min round |  42.46 min total |  5484.18 day sales |\n",
      "Predict | Day: 9\n",
      "##########  4.35 min round |  46.81 min total |  4229.58 day sales |\n",
      "Predict | Day: 10\n",
      "##########  4.35 min round |  51.16 min total |  4609.28 day sales |\n",
      "Predict | Day: 11\n",
      "##########  4.36 min round |  55.52 min total |  4568.74 day sales |\n",
      "Predict | Day: 12\n",
      "##########  4.27 min round |  59.78 min total |  5434.34 day sales |\n",
      "Predict | Day: 13\n",
      "##########  4.29 min round |  64.07 min total |  6386.81 day sales |\n",
      "Predict | Day: 14\n",
      "##########  4.35 min round |  68.42 min total |  6565.68 day sales |\n",
      "Predict | Day: 15\n",
      "##########  4.35 min round |  72.77 min total |  5219.44 day sales |\n",
      "Predict | Day: 16\n",
      "##########  4.37 min round |  77.14 min total |  4885.25 day sales |\n",
      "Predict | Day: 17\n",
      "##########  4.36 min round |  81.49 min total |  4573.36 day sales |\n",
      "Predict | Day: 18\n",
      "##########  4.36 min round |  85.85 min total |  4642.12 day sales |\n",
      "Predict | Day: 19\n",
      "##########  4.35 min round |  90.20 min total |  5379.09 day sales |\n",
      "Predict | Day: 20\n",
      "##########  4.35 min round |  94.55 min total |  6480.54 day sales |\n",
      "Predict | Day: 21\n",
      "##########  4.36 min round |  98.91 min total |  6700.72 day sales |\n",
      "Predict | Day: 22\n",
      "##########  4.37 min round |  103.28 min total |  4972.28 day sales |\n",
      "Predict | Day: 23\n",
      "##########  4.37 min round |  107.64 min total |  4320.26 day sales |\n",
      "Predict | Day: 24\n",
      "##########  4.36 min round |  112.01 min total |  4300.08 day sales |\n",
      "Predict | Day: 25\n",
      "##########  4.38 min round |  116.38 min total |  4393.41 day sales |\n",
      "Predict | Day: 26\n",
      "##########  4.27 min round |  120.66 min total |  5278.17 day sales |\n",
      "Predict | Day: 27\n",
      "##########  4.39 min round |  125.05 min total |  6447.65 day sales |\n",
      "Predict | Day: 28\n",
      "##########  4.40 min round |  129.44 min total |  6027.48 day sales |\n",
      "FOLD: 2\n",
      "Predict | Day: 1\n",
      "##########  4.42 min round |  4.42 min total |  4483.37 day sales |\n",
      "Predict | Day: 2\n",
      "##########  4.40 min round |  8.83 min total |  3973.67 day sales |\n",
      "Predict | Day: 3\n",
      "##########  4.44 min round |  13.26 min total |  3890.62 day sales |\n",
      "Predict | Day: 4\n",
      "##########  4.39 min round |  17.65 min total |  3935.87 day sales |\n",
      "Predict | Day: 5\n",
      "##########  4.25 min round |  21.90 min total |  4690.14 day sales |\n",
      "Predict | Day: 6\n",
      "##########  3.88 min round |  25.79 min total |  5893.55 day sales |\n",
      "Predict | Day: 7\n",
      "##########  3.89 min round |  29.68 min total |  6109.21 day sales |\n",
      "Predict | Day: 8\n",
      "##########  3.85 min round |  33.53 min total |  5331.96 day sales |\n",
      "Predict | Day: 9\n",
      "##########  3.81 min round |  37.34 min total |  4272.45 day sales |\n",
      "Predict | Day: 10\n",
      "##########  3.74 min round |  41.09 min total |  4709.54 day sales |\n",
      "Predict | Day: 11\n",
      "##########  3.68 min round |  44.77 min total |  4395.88 day sales |\n",
      "Predict | Day: 12\n",
      "##########  3.72 min round |  48.49 min total |  5579.90 day sales |\n",
      "Predict | Day: 13\n",
      "##########  3.69 min round |  52.18 min total |  6418.86 day sales |\n",
      "Predict | Day: 14\n",
      "##########  3.70 min round |  55.89 min total |  6604.44 day sales |\n",
      "Predict | Day: 15\n",
      "##########  3.96 min round |  59.84 min total |  5074.18 day sales |\n",
      "Predict | Day: 16\n",
      "##########  4.00 min round |  63.84 min total |  4589.89 day sales |\n",
      "Predict | Day: 17\n",
      "##########  4.01 min round |  67.86 min total |  4497.26 day sales |\n",
      "Predict | Day: 18\n",
      "##########  3.79 min round |  71.65 min total |  4572.77 day sales |\n",
      "Predict | Day: 19\n",
      "##########  3.83 min round |  75.48 min total |  5271.85 day sales |\n",
      "Predict | Day: 20\n",
      "##########  3.85 min round |  79.33 min total |  6295.82 day sales |\n",
      "Predict | Day: 21\n",
      "##########  3.84 min round |  83.17 min total |  6661.75 day sales |\n",
      "Predict | Day: 22\n",
      "##########  3.86 min round |  87.03 min total |  4881.40 day sales |\n",
      "Predict | Day: 23\n",
      "##########  3.86 min round |  90.88 min total |  4321.20 day sales |\n",
      "Predict | Day: 24\n",
      "##########  3.83 min round |  94.72 min total |  4340.13 day sales |\n",
      "Predict | Day: 25\n",
      "##########  3.82 min round |  98.54 min total |  4393.90 day sales |\n",
      "Predict | Day: 26\n",
      "##########  3.84 min round |  102.38 min total |  5221.20 day sales |\n",
      "Predict | Day: 27\n",
      "##########  3.51 min round |  105.88 min total |  6370.81 day sales |\n",
      "Predict | Day: 28\n",
      "##########  3.39 min round |  109.28 min total |  5928.80 day sales |\n"
     ]
    }
   ],
   "source": [
    "for fold_ in CV_FOLDS:\n",
    "    print(\"FOLD:\", fold_)\n",
    "    # Join back the Test dataset with \n",
    "    # a small part of the training data \n",
    "    # to make recursive features\n",
    "    all_preds = pd.DataFrame()\n",
    "    base_test = get_base_test()\n",
    "    # Timer to measure predictions time \n",
    "    main_time = time.time()\n",
    "\n",
    "    # Loop over each prediction day\n",
    "    # As rolling lags are the most timeconsuming\n",
    "    # we will calculate it for whole day\n",
    "    for PREDICT_DAY in range(1,29):    \n",
    "        print('Predict | Day:', PREDICT_DAY)\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Make temporary grid to calculate rolling lags\n",
    "        grid_df = base_test.copy()\n",
    "        lag_df_new = pd.DataFrame()\n",
    "        lag_df_new = make_lag_roll(ROLS_SPLIT,lag_df_new)\n",
    "        grid_df = grid_df.merge(lag_df_new, on=['id','d'], how='left')\n",
    "\n",
    "        for store_id in STORES_IDS:\n",
    "            \n",
    "            if store_id in ['CA_1', 'CA_2', 'CA_3','CA_4','TX_1','TX_2','TX_3']:\n",
    "                MODEL_FEATURES = ['item_id', 'dept_id', 'cat_id', 'release', 'sell_price', 'price_max', 'price_min',\n",
    "                                  'price_std', 'price_mean', 'price_norm', 'price_rank_dept', 'price_nunique', \n",
    "                                  'item_nunique', 'price_momentum', 'price_momentum_m', 'price_momentum_y', \n",
    "                                  'temperature_high', 'temperature_con', 'rainfall_m', 'event_name_1', \n",
    "                                  'event_type_1', 'event_name_2', 'event_type_2', 'snap_CA', 'snap_TX', \n",
    "                                  'snap_WI', 'is_first_half_month', 'event_bef_weekend', 'event_after_weekend', \n",
    "                                  'NBA', 'event_attention_after', 'event_attention_bef', 'event_attention_sum', \n",
    "                                  'tm_d', 'tm_w', 'tm_m', 'tm_q', 'tm_y', 'tm_wm', 'tm_dw', 'tm_w_end', 'enc_cat_id_mean',\n",
    "                                  'enc_cat_id_std', 'enc_dept_id_mean', 'enc_dept_id_std', 'enc_item_id_mean', 'enc_item_id_std',\n",
    "                                  'sales_lag_28', 'sales_lag_29', 'sales_lag_30', 'sales_lag_31', 'sales_lag_32', 'sales_lag_33', \n",
    "                                  'sales_lag_34', 'sales_lag_35', 'sales_lag_36', 'sales_lag_37', 'sales_lag_38', 'sales_lag_39',\n",
    "                                  'sales_lag_40', 'sales_lag_41', 'sales_lag_42', 'rolling_mean_7', 'rolling_std_7', 'rolling_mean_14', \n",
    "                                  'rolling_std_14', 'rolling_mean_28', 'rolling_std_28', 'rolling_mean_56', 'rolling_std_56', 'rolling_mean_168',\n",
    "                                  'rolling_std_168', 'rolling_mean_tmp_1_7', 'rolling_mean_tmp_1_14', 'rolling_mean_tmp_1_28', 'rolling_mean_tmp_1_56', \n",
    "                                  'rolling_mean_tmp_7_7', 'rolling_mean_tmp_7_14', 'rolling_mean_tmp_7_28', 'rolling_mean_tmp_7_56', \n",
    "                                  'rolling_mean_tmp_14_7', 'rolling_mean_tmp_14_14', 'rolling_mean_tmp_14_28', 'rolling_mean_tmp_14_56',\n",
    "                                  'rolling_quantile_97_28', 'rolling_quantile_87.5_28', 'rolling_quantile_50_28', \n",
    "                                  'rolling_quantile_22.5_28', 'rolling_quantile_3_28', 'rolling_quantile_97_56', \n",
    "                                  'rolling_quantile_87.5_56', 'rolling_quantile_50_56', 'rolling_quantile_22.5_56',\n",
    "                                  'rolling_quantile_3_56', 'rolling_quantile_97_168', 'rolling_quantile_87.5_168', \n",
    "                                  'rolling_quantile_50_168', 'rolling_quantile_22.5_168', 'rolling_quantile_3_168']\n",
    "            else:\n",
    "                MODEL_FEATURES = ['item_id', 'dept_id', 'cat_id', 'release', 'sell_price', 'price_max', 'price_min',\n",
    "                                  'price_std', 'price_mean', 'price_norm', 'price_rank_dept', 'price_nunique', \n",
    "                                  'item_nunique', 'price_momentum', 'price_momentum_m', 'price_momentum_y', \n",
    "                                  'temperature_high', 'temperature_con', 'rainfall_m','snow_m', 'event_name_1', \n",
    "                                  'event_type_1', 'event_name_2', 'event_type_2', 'snap_CA', 'snap_TX', \n",
    "                                  'snap_WI', 'is_first_half_month', 'event_bef_weekend', 'event_after_weekend', \n",
    "                                  'NBA', 'event_attention_after', 'event_attention_bef', 'event_attention_sum', \n",
    "                                  'tm_d', 'tm_w', 'tm_m', 'tm_q', 'tm_y', 'tm_wm', 'tm_dw', 'tm_w_end', 'enc_cat_id_mean',\n",
    "                                  'enc_cat_id_std', 'enc_dept_id_mean', 'enc_dept_id_std', 'enc_item_id_mean', 'enc_item_id_std',\n",
    "                                  'sales_lag_28', 'sales_lag_29', 'sales_lag_30', 'sales_lag_31', 'sales_lag_32', 'sales_lag_33', \n",
    "                                  'sales_lag_34', 'sales_lag_35', 'sales_lag_36', 'sales_lag_37', 'sales_lag_38', 'sales_lag_39',\n",
    "                                  'sales_lag_40', 'sales_lag_41', 'sales_lag_42', 'rolling_mean_7', 'rolling_std_7', 'rolling_mean_14', \n",
    "                                  'rolling_std_14', 'rolling_mean_28', 'rolling_std_28', 'rolling_mean_56', 'rolling_std_56', 'rolling_mean_168',\n",
    "                                  'rolling_std_168', 'rolling_mean_tmp_1_7', 'rolling_mean_tmp_1_14', 'rolling_mean_tmp_1_28', 'rolling_mean_tmp_1_56', \n",
    "                                  'rolling_mean_tmp_7_7', 'rolling_mean_tmp_7_14', 'rolling_mean_tmp_7_28', 'rolling_mean_tmp_7_56', \n",
    "                                  'rolling_mean_tmp_14_7', 'rolling_mean_tmp_14_14', 'rolling_mean_tmp_14_28', 'rolling_mean_tmp_14_56',\n",
    "                                  'rolling_quantile_97_28', 'rolling_quantile_87.5_28', 'rolling_quantile_50_28', \n",
    "                                  'rolling_quantile_22.5_28', 'rolling_quantile_3_28', 'rolling_quantile_97_56', \n",
    "                                  'rolling_quantile_87.5_56', 'rolling_quantile_50_56', 'rolling_quantile_22.5_56',\n",
    "                                  'rolling_quantile_3_56', 'rolling_quantile_97_168', 'rolling_quantile_87.5_168', \n",
    "                                  'rolling_quantile_50_168', 'rolling_quantile_22.5_168', 'rolling_quantile_3_168']\n",
    "        # Read all our models and make predictions\n",
    "        \n",
    "            # Read all our models and make predictions\n",
    "            # for each day/store pairs\n",
    "            model_path = 'lgb_model_'+store_id+'_'+str(fold_)+'.bin' \n",
    "\n",
    "            estimator = pickle.load(open(model_path, 'rb'))\n",
    "\n",
    "            day_mask = base_test['d']==(END_TRAIN+PREDICT_DAY)\n",
    "            store_mask = base_test['store_id']==store_id\n",
    "\n",
    "            mask = (day_mask)&(store_mask)\n",
    "            base_test[TARGET][mask] = estimator.predict(grid_df[mask][MODEL_FEATURES])\n",
    "\n",
    "        # Make good column naming and add \n",
    "        # to all_preds DataFrame\n",
    "        temp_df = base_test[day_mask][['id',TARGET]]\n",
    "        temp_df.columns = ['id','F'+str(PREDICT_DAY)]\n",
    "        if 'id' in list(all_preds):\n",
    "            all_preds = all_preds.merge(temp_df, on=['id'], how='left')\n",
    "        else:\n",
    "            all_preds = temp_df.copy()\n",
    "        all_preds = all_preds.reset_index(drop=True)\n",
    "        print('#'*10, ' %0.2f min round |' % ((time.time() - start_time) / 60),\n",
    "                      ' %0.2f min total |' % ((time.time() - main_time) / 60),\n",
    "                      ' %0.2f day sales |' % (temp_df['F'+str(PREDICT_DAY)].sum()))\n",
    "    all_preds.to_csv('model4_CA_1'+str(fold_)+'.csv',index=False)\n",
    "    del temp_df, all_preds, lag_df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
